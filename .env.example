# Context Rot Research - API Keys and Configuration Template
# Copy this file to .env and fill in your actual API keys
# SECURITY: Never commit .env file - it's in .gitignore

# ============================================================================
# OpenAI API (for OpenAI models + LLM judge)
# ============================================================================
OPENAI_API_KEY=sk-your-openai-api-key-here

# ============================================================================
# Anthropic API
# ============================================================================
ANTHROPIC_API_KEY=sk-ant-your-anthropic-api-key-here

# ============================================================================
# Google Cloud AI Platform
# ============================================================================
GOOGLE_APPLICATION_CREDENTIALS=/path/to/your/service-account-key.json
GOOGLE_MODEL_PATH=projects/YOUR_PROJECT/locations/us-central1/publishers/google/models/

# ============================================================================
# GPT-OSS / Qwen Models Configuration
# ============================================================================
#
# DEPLOYMENT MODE QUICK START:
#
# For 20b model (local deployment recommended):
#   1. Install ollama: https://ollama.ai
#   2. Run: ollama run qwen2.5:20b --num_ctx 131072
#   3. Set: GPT_OSS_BASE_URL=http://localhost:11434/v1
#   4. In scripts/run_full_research.sh: DEPLOY_20B="local"
#
# For 120b model (cloud deployment recommended):
#   1. Get OpenRouter API key: https://openrouter.ai
#   2. Set: OPENROUTER_API_KEY=sk-or-...
#   3. Set: OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
#   4. In scripts/run_full_research.sh: DEPLOY_120B="cloud"
#
# ============================================================================

# Option 1: Local Deployment (vLLM, ollama, text-generation-inference)
GPT_OSS_BASE_URL=http://localhost:11434/v1  # ollama default
# GPT_OSS_BASE_URL=http://localhost:8000/v1  # vLLM default
GPT_OSS_API_KEY=dummy  # Optional, some local servers don't require auth

# Option 2: OpenRouter (recommended for 120b model)
# OPENROUTER_API_KEY=sk-or-your-openrouter-key-here
# OPENROUTER_BASE_URL=https://openrouter.ai/api/v1

# Option 3: Use OpenAI API directly (set OPENAI_API_KEY above)
# GPT_OSS_BASE_URL=https://api.openai.com/v1

# Provider Preference (optional - overrides auto-detection)
# Set to explicitly choose which API to use for GPT-OSS models
# GPT_OSS_PREFER_PROVIDER=openai     # Use OpenAI API directly
# GPT_OSS_PREFER_PROVIDER=openrouter # Use OpenRouter
# GPT_OSS_PREFER_PROVIDER=local      # Use local deployment
# If not set, auto-detects with priority: OpenAI > OpenRouter > local

# Model Specifications (Qwen standard)
# - Context length: 128k tokens (131,072 tokens)
# - Models: qwen2.5:20b (local), qwen/qwen-2.5-72b-instruct (cloud)

# ============================================================================
# Setup Instructions
# ============================================================================
# 1. Copy this file: cp .env.example .env
# 2. Fill in your API keys in .env
# 3. Set permissions: chmod 600 .env
# 4. Verify .env is in .gitignore (already configured)
